# 
# this is the llama_cpp:samples container (built on top of llama_cpp)
# see Dockerfile & config.py for package configuration/metadata
#

ARG BASE_IMAGE
FROM ${BASE_IMAGE}

ARG LLAMA_CPP_VERSION \
    LLAMA_CPP_BRANCH

COPY build.sh /tmp/build_llama_cpp.sh
COPY benchmark.py /usr/local/bin/llama_cpp_benchmark.py

RUN pip3 install --no-cache-dir --verbose \
        typing-extensions \
        uvicorn \
        anyio \
        starlette \
        sse-starlette \
        starlette-context \
        fastapi \
        pydantic-settings && \
#    pip3 install --no-cache-dir --verbose llama-cpp-python==${LLAMA_CPP_VERSION} || \
    /tmp/build_llama_cpp.sh

RUN pip3 install gguf

RUN cp -r /opt/llama-cpp-python/vendor/llama.cpp/ /opt/llama.cpp/
        
CMD /start_ollama && \
    JUPYTER_ROOT=/data/notebooks/llama_cpp /start_jupyter && \
    /bin/bash